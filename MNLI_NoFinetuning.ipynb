{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import BertForSequenceClassification, BertTokenizer, Trainer, TrainingArguments\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\uchau\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import BertForSequenceClassification, BertTokenizer, Trainer, TrainingArguments\n",
    "from datasets import load_metric\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "import numpy as np\n",
    "\n",
    "model_name = 'textattack/bert-base-uncased-MNLI'\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = BertForSequenceClassification.from_pretrained(model_name, num_labels=3)  # MNLI has 3 labels: entailment, neutral, contradiction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "mnli = load_dataset('glue', 'mnli')\n",
    "\n",
    "def preprocess(examples):\n",
    "    return tokenizer(examples['premise'], examples['hypothesis'], truncation=True, padding='max_length', max_length=128)\n",
    "\n",
    "encoded_dataset = mnli.map(preprocess, batched=True)\n",
    "\n",
    "# Set the format for PyTorch\n",
    "encoded_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\uchau\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\datasets\\load.py:759: FutureWarning: The repository for accuracy contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.19.1/metrics/accuracy/accuracy.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "import numpy as np\n",
    "from datasets import load_metric\n",
    "\n",
    "# Define the metric and trainer\n",
    "accuracy_metric = load_metric(\"accuracy\")\n",
    "\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    preds = np.argmax(predictions, axis=1)\n",
    "    acc = accuracy_metric.compute(predictions=preds, references=labels)\n",
    "    f1_weighted = f1_score(labels, preds, average='weighted')\n",
    "    f1_per_class = f1_score(labels, preds, average=None)\n",
    "    return {'accuracy': acc['accuracy'], 'f1_weighted': f1_weighted, 'f1_per_class': f1_per_class}\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',          # output directory\n",
    "    per_device_eval_batch_size=16,   # batch size for evaluation\n",
    "    do_train=False,                  # don't perform training\n",
    "    do_eval=True,                    # perform evaluation\n",
    "    logging_dir='./logs',            # directory for storing logs\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,                     # the pre-trained model\n",
    "    args=training_args,              # training arguments\n",
    "    eval_dataset=encoded_dataset['validation_matched'], # evaluation dataset\n",
    "    compute_metrics=compute_metrics  # evaluation metric function\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31a6d9487a00439aadf1dbeff41cab9a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/614 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer is attempting to log a value of \"[0.04289277 0.3900935  0.24649659]\" of type <class 'numpy.ndarray'> for key \"eval/f1_per_class\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n"
     ]
    }
   ],
   "source": [
    "eval_result = trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 27.8%\n",
      "Weighted F1 Score: 22.0%\n",
      "F1 Score for each class: [ 4.28927681 39.00934951 24.64965864]\n"
     ]
    }
   ],
   "source": [
    "acc = eval_result['eval_accuracy'] * 100\n",
    "f1_weighted = eval_result['eval_f1_weighted'] * 100\n",
    "f1_per_class = eval_result['eval_f1_per_class'] * 100\n",
    "\n",
    "print(f\"Accuracy: {acc:.1f}%\")\n",
    "print(f\"Weighted F1 Score: {f1_weighted:.1f}%\")\n",
    "print(f\"F1 Score for each class: {f1_per_class}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
