{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertForSequenceClassification, BertTokenizer, Trainer, TrainingArguments\n",
    "from datasets import Dataset, load_metric\n",
    "import pandas as pd\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\uchau\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\uchau\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    }
   ],
   "source": [
    "model_name = 'Abby-OGV/circa_mnli_yn'\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = BertForSequenceClassification.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "dataset = pd.read_parquet('Data/Circa_train.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter and map the labels\n",
    "label_map = {\n",
    "    0: 0,\n",
    "    1: 1,\n",
    "    2: 2,\n",
    "    3: 3\n",
    "}\n",
    "filtered_dataset = dataset[dataset['goldstandard2'].isin([0, 1, 2, 3])]\n",
    "#filtered_dataset['goldstandard2'] = filtered_dataset['goldstandard2'].map(label_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data\n",
    "train_data, dev_data = train_test_split(filtered_dataset, test_size=0.4, random_state=42)\n",
    "dev_data, test_data = train_test_split(dev_data, test_size=0.5, random_state=42)\n",
    "\n",
    "# Convert pandas DataFrame to Hugging Face Dataset\n",
    "train_dataset = Dataset.from_pandas(train_data)\n",
    "dev_dataset = Dataset.from_pandas(dev_data)\n",
    "test_dataset = Dataset.from_pandas(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "063f8c89b6404e16944b420eab09b6a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/19795 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09907e8dfd2e4ded9975d44be294d832",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6599 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56a0e02ff78246bba324827812c99635",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6599 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\uchau\\AppData\\Local\\Temp\\ipykernel_28132\\3174071378.py:13: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate\n",
      "  accuracy_metric = load_metric(\"accuracy\")\n",
      "c:\\Users\\uchau\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\datasets\\load.py:759: FutureWarning: The repository for accuracy contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.19.1/metrics/accuracy/accuracy.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Preprocess function\n",
    "def preprocess(examples):\n",
    "    tokenized_inputs = tokenizer(examples['question-X'], examples['answer-Y'], truncation=True, padding='max_length', max_length=128)\n",
    "    tokenized_inputs['labels'] = examples['goldstandard2']\n",
    "    return tokenized_inputs\n",
    "\n",
    "# Apply preprocessing\n",
    "encoded_train_dataset = train_dataset.map(preprocess, batched=True)\n",
    "encoded_dev_dataset = dev_dataset.map(preprocess, batched=True)\n",
    "encoded_test_dataset = test_dataset.map(preprocess, batched=True)\n",
    "\n",
    "# Load metrics\n",
    "accuracy_metric = load_metric(\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute metrics function\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    preds = np.argmax(predictions, axis=1)\n",
    "    acc = accuracy_metric.compute(predictions=preds, references=labels)\n",
    "    f1_weighted = f1_score(labels, preds, average='weighted')\n",
    "    f1_per_class = f1_score(labels, preds, average=None)\n",
    "    return {\n",
    "        'accuracy': acc['accuracy'],\n",
    "        'f1_weighted': f1_weighted,\n",
    "        'f1_per_class': f1_per_class\n",
    "    }\n",
    "\n",
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    per_device_eval_batch_size=16,\n",
    "    do_train=False,\n",
    "    do_eval=True,\n",
    "    logging_dir='./logs'\n",
    ")\n",
    "\n",
    "# Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    eval_dataset=encoded_test_dataset,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "802e122b823e42449b3847efa72e758b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/413 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer is attempting to log a value of \"[0.91112419 0.9007755  0.35       0.93067426]\" of type <class 'numpy.ndarray'> for key \"eval/f1_per_class\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "eval_result = trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 89.5%\n",
      "Weighted F1 Score: 89.2%\n",
      "F1 Score for class 0: 91.11%\n",
      "F1 Score for class 1: 90.08%\n",
      "F1 Score for class 2: 35.00%\n",
      "F1 Score for class 3: 93.07%\n"
     ]
    }
   ],
   "source": [
    "# Print results\n",
    "acc = eval_result.get('eval_accuracy', None) * 100 if 'eval_accuracy' in eval_result else None\n",
    "f1_weighted = eval_result.get('eval_f1_weighted', None) * 100 if 'eval_f1_weighted' in eval_result else None\n",
    "f1_per_class = eval_result.get('eval_f1_per_class', None) * 100 if 'eval_f1_per_class' in eval_result else None\n",
    "\n",
    "print(f\"Accuracy: {acc:.1f}%\" if acc is not None else \"Accuracy not found\")\n",
    "print(f\"Weighted F1 Score: {f1_weighted:.1f}%\" if f1_weighted is not None else \"Weighted F1 Score not found\")\n",
    "\n",
    "if f1_per_class is not None:\n",
    "    for i, score in enumerate(f1_per_class):\n",
    "        print(f\"F1 Score for class {i}: {score:.2f}%\")\n",
    "else:\n",
    "    print(\"F1 Score for each class not found\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
